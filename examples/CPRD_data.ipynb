{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pds\n",
    "import numpy as np\n",
    "import time\n",
    "import importlib\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdiffeq import odeint_adjoint as odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdiffeq import odeint as dto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ode_models' from '/alt/applic/user-maint/zq224/WS/torchdiffeq/examples/ode_models.py'>"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ode_models\n",
    "importlib.reload(ode_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'training_utils' from '/alt/applic/user-maint/zq224/WS/torchdiffeq/examples/training_utils.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import baseline_models as baseline\n",
    "import training_utils\n",
    "\n",
    "importlib.reload(training_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:' + str(1) if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_TYPE = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_dict = training_utils.get_data('data/cprd_full_4_markers.csv.gz')\n",
    "dat_folds = training_utils.get_fold(dat_dict, fold=5, seed=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_dict['1025']['t'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 1, 4])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_dict['1025']['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, y, y0, t_mask, y_mask, eids = training_utils.get_batch(dat_folds[0]['train'], batch_size=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 70, 1, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9626)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t, y, y0, t_mask, y_mask, eids = get_batch(dat_dict, batch_size=7000)\n",
    "torch.mean(torch.abs(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 1, 4])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM without time info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'training_utils' from '/alt/applic/user-maint/zq224/WS/torchdiffeq/examples/training_utils.py'>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 1000\n",
    "test_freq = 100\n",
    "batch_size = 500\n",
    "input_dim = output_dim = 4\n",
    "n_hidden = 50\n",
    "\n",
    "base_lstm = baseline.BaselineLSTM(input_dim, n_hidden, output_dim)\n",
    "optimizer = optim.Adam(base_lstm.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_lstm_loss_func(t, y, y0, t_mask, y_mask, eids):\n",
    "    y_pred = base_lstm(y0, t)\n",
    "    loss = torch.mean(torch.abs(y_pred[y_mask.squeeze()] - y[y_mask]))\n",
    "    return loss\n",
    "\n",
    "def base_lstm_save_func():\n",
    "    model_path = 'models/cprd_lstm_no_time.pth'\n",
    "    torch.save(base_lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0100 | Total Loss 0.527168\n",
      "Iter 0200 | Total Loss 0.452818\n",
      "Iter 0300 | Total Loss 0.420922\n",
      "Iter 0400 | Total Loss 0.401545\n",
      "Iter 0500 | Total Loss 0.389461\n",
      "Iter 0600 | Total Loss 0.382782\n",
      "Iter 0700 | Total Loss 0.378658\n",
      "Iter 0800 | Total Loss 0.376341\n",
      "Iter 0900 | Total Loss 0.373735\n",
      "Iter 1000 | Total Loss 0.371715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3717), 62.42432188987732)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_utils.training_loop(niters, \n",
    "                    dat_folds[0], \n",
    "                    batch_size, \n",
    "                    optimizer, \n",
    "                    test_freq, \n",
    "                    base_lstm_loss_func, \n",
    "                    base_lstm_save_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM with time info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 5000\n",
    "test_freq = 100\n",
    "batch_size = 500\n",
    "input_dim = output_dim = 4\n",
    "n_hidden = 50\n",
    "model_path = 'models/cprd_lstm.pth'\n",
    "\n",
    "base_time_lstm = baseline.BaselineTimeLSTM(input_dim, n_hidden, output_dim)\n",
    "\n",
    "optimizer = optim.Adam(base_time_lstm.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_time_lstm_loss_func(t, y, y0, t_mask, y_mask, eids):\n",
    "    y_pred = base_time_lstm(y0, t)\n",
    "    loss = torch.mean(torch.abs(y_pred[y_mask.squeeze()] - y[y_mask]))\n",
    "    return loss\n",
    "\n",
    "def base_time_lstm_save_func():\n",
    "    model_path = 'models/cprd_lstm.pth'\n",
    "    torch.save(base_time_lstm.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0100 | Total Loss 0.525657\n",
      "Iter 0200 | Total Loss 0.447242\n",
      "Iter 0300 | Total Loss 0.412594\n",
      "Iter 0400 | Total Loss 0.391881\n",
      "Iter 0500 | Total Loss 0.384577\n",
      "Iter 0600 | Total Loss 0.378533\n",
      "Iter 0700 | Total Loss 0.374993\n",
      "Iter 0800 | Total Loss 0.372451\n",
      "Iter 0900 | Total Loss 0.370759\n",
      "Iter 1000 | Total Loss 0.369180\n",
      "Iter 1100 | Total Loss 0.368492\n",
      "Iter 1200 | Total Loss 0.368420\n",
      "Iter 1300 | Total Loss 0.367803\n",
      "Iter 1400 | Total Loss 0.367126\n",
      "Iter 1500 | Total Loss 0.367088\n",
      "Iter 1600 | Total Loss 0.366410\n",
      "Iter 1700 | Total Loss 0.366708\n",
      "Iter 1800 | Total Loss 0.366041\n",
      "Iter 1900 | Total Loss 0.365523\n",
      "Iter 2000 | Total Loss 0.365653\n",
      "Iter 2100 | Total Loss 0.365256\n",
      "Iter 2200 | Total Loss 0.364898\n",
      "Iter 2300 | Total Loss 0.365341\n",
      "Iter 2400 | Total Loss 0.364773\n",
      "Iter 2500 | Total Loss 0.364827\n",
      "Iter 2600 | Total Loss 0.364687\n",
      "Iter 2700 | Total Loss 0.363944\n",
      "Iter 2800 | Total Loss 0.363720\n",
      "Iter 2900 | Total Loss 0.364264\n",
      "Iter 3000 | Total Loss 0.363748\n",
      "Iter 3100 | Total Loss 0.363850\n",
      "Iter 3200 | Total Loss 0.363424\n",
      "Iter 3300 | Total Loss 0.363756\n",
      "Iter 3400 | Total Loss 0.363236\n",
      "Iter 3500 | Total Loss 0.363238\n",
      "Iter 3600 | Total Loss 0.363446\n",
      "Iter 3700 | Total Loss 0.362917\n",
      "Iter 3800 | Total Loss 0.362907\n",
      "Iter 3900 | Total Loss 0.362970\n",
      "Iter 4000 | Total Loss 0.362961\n",
      "Iter 4100 | Total Loss 0.362657\n",
      "Iter 4200 | Total Loss 0.362742\n",
      "Iter 4300 | Total Loss 0.363058\n",
      "Iter 4400 | Total Loss 0.363310\n",
      "Iter 4500 | Total Loss 0.362361\n",
      "Iter 4600 | Total Loss 0.362552\n",
      "Iter 4700 | Total Loss 0.362318\n",
      "Iter 4800 | Total Loss 0.362919\n",
      "Iter 4900 | Total Loss 0.362201\n",
      "Iter 5000 | Total Loss 0.362750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3622), 267.44750809669495)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_utils.training_loop(niters, \n",
    "                    dat_folds[0], \n",
    "                    batch_size, \n",
    "                    optimizer, \n",
    "                    test_freq, \n",
    "                    base_time_lstm_loss_func, \n",
    "                    base_time_lstm_save_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineTimeLSTM(\n",
       "  (lstm): LSTM(5, 50)\n",
       "  (lin): Linear(in_features=50, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_time_lstm = baseline.BaselineTimeLSTM(input_dim, n_hidden, output_dim)\n",
    "base_time_lstm.load_state_dict(torch.load('models/cprd_lstm.pth'))\n",
    "base_time_lstm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_loss = base_time_lstm_loss_func(*training_utils.get_all(dat_folds[0]['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3622, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla Neural ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 1000\n",
    "batch_size = 500\n",
    "step_size = 1./12\n",
    "test_freq = 50\n",
    "\n",
    "# vanila Neural ODE\n",
    "func0 = ode_models.ODEFunc0(dim_y=4)\n",
    "optimizer = optim.Adam(func0.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vanilla_ode_loss_func(t, y, y0, t_mask, y_mask, eids):\n",
    "    pred_y = dto(func0, y0, t, method='euler_par', options={'step_size': step_size})\n",
    "    loss = torch.mean(torch.abs(pred_y[y_mask] - y[y_mask]))\n",
    "    return loss\n",
    "\n",
    "def Vanilla_ode_save_func():\n",
    "    model_path = 'models/vanilla_ode.pth'\n",
    "    torch.save(func0.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0050 | Total Loss 0.401434\n"
     ]
    }
   ],
   "source": [
    "training_utils.training_loop(niters, \n",
    "                    dat_folds[0], \n",
    "                    batch_size, \n",
    "                    optimizer, \n",
    "                    test_freq, \n",
    "                    Vanilla_ode_loss_func, \n",
    "                    Vanilla_ode_save_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ODEFunc0(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=50, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=50, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func0 = ode_models.ODEFunc0(dim_y=4)\n",
    "func0.load_state_dict(torch.load('models/vanilla_ode.pth'))\n",
    "func0.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_loss = Vanilla_ode_loss_func(*training_utils.get_all(dat_folds[0]['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3707, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ode_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 2000\n",
    "batch_size = 500\n",
    "step_size = 1./12\n",
    "test_freq = 50\n",
    "\n",
    "# augmented Neural ODE\n",
    "func_aug = ode_models.ODEFuncAug(dim_y=4, dim_aug=4)\n",
    "optimizer = optim.Adam(func_aug.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmented_ode_loss_func(t, y, y0, t_mask, y_mask, eids):\n",
    "    y0_aug = F.pad(y0, (0, func_aug.dim_aug, 0, 0, 0, 0), \"constant\", 0.)\n",
    "    pred_y = dto(func_aug, y0_aug, t, method='euler_par', options={'step_size': step_size})\n",
    "    pred_y = pred_y[..., :func_aug.dim_y]\n",
    "    loss = torch.mean(torch.abs(pred_y[y_mask] - y[y_mask]))\n",
    "    return loss\n",
    "\n",
    "def augmented_ode_save_func():\n",
    "    model_path = 'models/augmented_ode.pth'\n",
    "    torch.save(func_aug.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0050 | Total Loss 0.368088\n",
      "Iter 0100 | Total Loss 0.367508\n",
      "Iter 0150 | Total Loss 0.367459\n",
      "Iter 0200 | Total Loss 0.367632\n",
      "Iter 0250 | Total Loss 0.367057\n",
      "Iter 0300 | Total Loss 0.367356\n",
      "Iter 0350 | Total Loss 0.366467\n",
      "Iter 0400 | Total Loss 0.366346\n",
      "Iter 0450 | Total Loss 0.366428\n",
      "Iter 0500 | Total Loss 0.366202\n",
      "Iter 0550 | Total Loss 0.366959\n",
      "Iter 0600 | Total Loss 0.365873\n",
      "Iter 0650 | Total Loss 0.366604\n",
      "Iter 0700 | Total Loss 0.365845\n",
      "Iter 0750 | Total Loss 0.365741\n",
      "Iter 0800 | Total Loss 0.365889\n",
      "Iter 0850 | Total Loss 0.365845\n",
      "Iter 0900 | Total Loss 0.366190\n",
      "Iter 0950 | Total Loss 0.365997\n",
      "Iter 1000 | Total Loss 0.365611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3656), 1469.1548671722412)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_utils.training_loop(niters, \n",
    "                    dat_folds[0], \n",
    "                    batch_size, \n",
    "                    optimizer, \n",
    "                    test_freq, \n",
    "                    augmented_ode_loss_func, \n",
    "                    augmented_ode_save_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ODEFuncAug(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=50, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_aug = ode_models.ODEFuncAug(dim_y=4, dim_aug=4)\n",
    "func_aug.load_state_dict(torch.load('models/augmented_ode.pth'))\n",
    "func_aug.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3653, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_loss = augmented_ode_loss_func(*training_utils.get_all(dat_folds[0]['test']))\n",
    "aug_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-Order ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 5000\n",
    "batch_size = 500\n",
    "step_size = 1./12\n",
    "test_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = ode_models.HigherOrderOde(dat_dict, batch_size=batch_size, dim=4, order=2, hidden_size=50)\n",
    "func.init_cond_mat.requires_grad = False\n",
    "optimizer = optim.Adam(func.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def higher_ode_loss_func(t, y, y0, t_mask, y_mask, eids):\n",
    "    func.set_init_cond(eids)\n",
    "    init_zeros = torch.zeros_like(func.init_cond)\n",
    "    \n",
    "    pred_y = dto(func, init_zeros, t, method='euler_par', options={'step_size': step_size})\n",
    "    pred_y_final = (pred_y  + func.init_cond)[..., :func.dim]\n",
    "    \n",
    "    loss = torch.mean(torch.abs(pred_y_final[y_mask] - y[y_mask]))\n",
    "    return loss\n",
    "\n",
    "def higher_ode_save_func():\n",
    "    model_path = 'models/higher_ode.pth'\n",
    "    torch.save(func.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0050 | Total Loss 0.607941\n",
      "Iter 0100 | Total Loss 0.602815\n",
      "Iter 0150 | Total Loss 0.598663\n",
      "Iter 0200 | Total Loss 0.595352\n",
      "Iter 0250 | Total Loss 0.591374\n",
      "Iter 0300 | Total Loss 0.587983\n",
      "Iter 0350 | Total Loss 0.584539\n",
      "Iter 0400 | Total Loss 0.581387\n",
      "Iter 0450 | Total Loss 0.578409\n",
      "Iter 0500 | Total Loss 0.575506\n",
      "Iter 0550 | Total Loss 0.572736\n",
      "Iter 0600 | Total Loss 0.569395\n",
      "Iter 0650 | Total Loss 0.566920\n",
      "Iter 0700 | Total Loss 0.564475\n",
      "Iter 0750 | Total Loss 0.561346\n",
      "Iter 0800 | Total Loss 0.558838\n",
      "Iter 0850 | Total Loss 0.556566\n",
      "Iter 0900 | Total Loss 0.553566\n",
      "Iter 0950 | Total Loss 0.551305\n",
      "Iter 1000 | Total Loss 0.548526\n",
      "Iter 1050 | Total Loss 0.545518\n",
      "Iter 1100 | Total Loss 0.543831\n",
      "Iter 1150 | Total Loss 0.541061\n",
      "Iter 1200 | Total Loss 0.539124\n",
      "Iter 1250 | Total Loss 0.537687\n",
      "Iter 1300 | Total Loss 0.536138\n",
      "Iter 1350 | Total Loss 0.534140\n",
      "Iter 1400 | Total Loss 0.532297\n",
      "Iter 1450 | Total Loss 0.530554\n",
      "Iter 1500 | Total Loss 0.529012\n",
      "Iter 1550 | Total Loss 0.527009\n",
      "Iter 1600 | Total Loss 0.525146\n",
      "Iter 1650 | Total Loss 0.523554\n",
      "Iter 1700 | Total Loss 0.521529\n",
      "Iter 1750 | Total Loss 0.520015\n",
      "Iter 1800 | Total Loss 0.518538\n",
      "Iter 1850 | Total Loss 0.516170\n",
      "Iter 1900 | Total Loss 0.514676\n",
      "Iter 1950 | Total Loss 0.512869\n",
      "Iter 2000 | Total Loss 0.510889\n",
      "Iter 2050 | Total Loss 0.509463\n",
      "Iter 2100 | Total Loss 0.508214\n",
      "Iter 2150 | Total Loss 0.506490\n",
      "Iter 2200 | Total Loss 0.504625\n",
      "Iter 2250 | Total Loss 0.503488\n",
      "Iter 2300 | Total Loss 0.501207\n",
      "Iter 2350 | Total Loss 0.499988\n",
      "Iter 2400 | Total Loss 0.498370\n",
      "Iter 2450 | Total Loss 0.496844\n",
      "Iter 2500 | Total Loss 0.494920\n",
      "Iter 2550 | Total Loss 0.493879\n",
      "Iter 2600 | Total Loss 0.492355\n",
      "Iter 2650 | Total Loss 0.491558\n",
      "Iter 2700 | Total Loss 0.490371\n",
      "Iter 2750 | Total Loss 0.488689\n",
      "Iter 2800 | Total Loss 0.487196\n",
      "Iter 2850 | Total Loss 0.486207\n",
      "Iter 2900 | Total Loss 0.484748\n",
      "Iter 2950 | Total Loss 0.483424\n",
      "Iter 3000 | Total Loss 0.482292\n",
      "Iter 3050 | Total Loss 0.481392\n",
      "Iter 3100 | Total Loss 0.479918\n",
      "Iter 3150 | Total Loss 0.478496\n",
      "Iter 3200 | Total Loss 0.477327\n",
      "Iter 3250 | Total Loss 0.476429\n",
      "Iter 3300 | Total Loss 0.474759\n",
      "Iter 3350 | Total Loss 0.474147\n",
      "Iter 3400 | Total Loss 0.472419\n",
      "Iter 3450 | Total Loss 0.470616\n",
      "Iter 3500 | Total Loss 0.469400\n",
      "Iter 3550 | Total Loss 0.469179\n",
      "Iter 3600 | Total Loss 0.468463\n",
      "Iter 3650 | Total Loss 0.466754\n",
      "Iter 3700 | Total Loss 0.465948\n",
      "Iter 3750 | Total Loss 0.465182\n",
      "Iter 3800 | Total Loss 0.463747\n",
      "Iter 3850 | Total Loss 0.462764\n",
      "Iter 3900 | Total Loss 0.461486\n",
      "Iter 3950 | Total Loss 0.460624\n",
      "Iter 4000 | Total Loss 0.459379\n",
      "Iter 4050 | Total Loss 0.459156\n",
      "Iter 4100 | Total Loss 0.457565\n",
      "Iter 4150 | Total Loss 0.457476\n",
      "Iter 4200 | Total Loss 0.457056\n",
      "Iter 4250 | Total Loss 0.456087\n",
      "Iter 4300 | Total Loss 0.455418\n",
      "Iter 4350 | Total Loss 0.454296\n",
      "Iter 4400 | Total Loss 0.453457\n",
      "Iter 4450 | Total Loss 0.452789\n",
      "Iter 4500 | Total Loss 0.452312\n",
      "Iter 4550 | Total Loss 0.451644\n",
      "Iter 4600 | Total Loss 0.451124\n",
      "Iter 4650 | Total Loss 0.449709\n",
      "Iter 4700 | Total Loss 0.448706\n",
      "Iter 4750 | Total Loss 0.448985\n",
      "Iter 4800 | Total Loss 0.447836\n",
      "Iter 4850 | Total Loss 0.446043\n",
      "Iter 4900 | Total Loss 0.444794\n",
      "Iter 4950 | Total Loss 0.444395\n",
      "Iter 5000 | Total Loss 0.444129\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.4441), 6289.509413957596)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_utils.training_loop(niters, \n",
    "                    dat_folds[0], \n",
    "                    batch_size, \n",
    "                    optimizer, \n",
    "                    test_freq, \n",
    "                    higher_ode_loss_func, \n",
    "                    higher_ode_save_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HigherOrderOde(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=50, bias=True)\n",
       "    (1): Tanh()\n",
       "    (2): Linear(in_features=50, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = ode_models.HigherOrderOde(dat_dict, batch_size=batch_size, dim=4, order=2, hidden_size=50)\n",
    "func.load_state_dict(torch.load('models/higher_ode.pth'))\n",
    "func.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4407, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ho_ode_loss = higher_ode_loss_func(*training_utils.get_all(dat_folds[0]['test']))\n",
    "ho_ode_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1412, -0.0645, -0.1679, -0.0811, -0.0933,  0.1038, -0.0642,  0.1190,\n",
       "         0.3571,  0.0430, -0.0459,  0.0796, -0.0195,  0.0265, -0.1238, -0.0574,\n",
       "         0.0315,  0.0433, -0.0834,  0.0632,  0.1109, -0.0758, -0.6604, -0.0978,\n",
       "         0.0470,  0.0435, -0.1130, -0.1145, -0.1188, -0.0500,  0.0768,  0.1041,\n",
       "         0.0215, -0.0188,  0.0577, -0.1248, -0.0044,  0.0667,  0.0290,  0.0102,\n",
       "         0.0444,  0.0517,  0.0658, -0.1371,  0.1418, -0.1468,  0.0901, -0.0873,\n",
       "        -0.1132, -0.0102], requires_grad=True)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.net[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "negative step not yet supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-9aec9ea1ebc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: negative step not yet supported"
     ]
    }
   ],
   "source": [
    "func.net[0].bias[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## latent ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run backward lstm to infer initial condition\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(input_dim + 1, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to output space\n",
    "        self.lin = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, y, t):\n",
    "        # y and t are the first k observations\n",
    "        \n",
    "        batch_size = y.shape[1]\n",
    "        dim_y = y.shape[-1]\n",
    "        t_max = t.shape[0]\n",
    "        \n",
    "        t = t.view((t_max, batch_size, 1))\n",
    "        y = y.view((t_max, batch_size, dim_y))\n",
    "        \n",
    "        y_in = torch.cat((y, t), dim=-1)\n",
    "\n",
    "        hidden = None\n",
    "        \n",
    "        for t in reversed(range(t_max)):\n",
    "            obs = y_in[t:t+1, ...]\n",
    "            out, hidden = self.lstm(obs, hidden)\n",
    "        out_linear = self.lin(out)\n",
    "        \n",
    "        return out_linear.permute((1, 0, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim=4, obs_dim=2, nhidden=20):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc1 = nn.Linear(latent_dim, nhidden)\n",
    "        self.fc2 = nn.Linear(nhidden, obs_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.fc1(z)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 1000\n",
    "batch_size = 500\n",
    "step_size = 1./12\n",
    "test_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(4, 50, 10)\n",
    "latent_ode = ode_models.ODEFunc0(dim_y=10)\n",
    "decoder = Decoder(10, 4, 20)\n",
    "\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(latent_ode.parameters()) + list(decoder.parameters()), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_ode_loss_func(t, y, y0, t_mask, y_mask, eids):\n",
    "    init_cond = encoder(y[:1, ...], t[:1])\n",
    "    # y_mask[:2, ...] = False\n",
    "    latent_y = dto(latent_ode, init_cond, t, method='euler_par', options={'step_size': step_size})\n",
    "    pred_y = decoder(latent_y)\n",
    "    loss = torch.mean(torch.abs(pred_y[y_mask] - y[y_mask]))\n",
    "    return loss\n",
    "\n",
    "def enc_dec_ode_save_func():\n",
    "    model_path = 'models/enc-dec-{}.pth'\n",
    "    torch.save(encoder.state_dict(), model_path.format('encoder'))\n",
    "    torch.save(decoder.state_dict(), model_path.format('decoder'))\n",
    "    torch.save(latent_ode.state_dict(), model_path.format('ode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0050 | Total Loss 0.367592\n",
      "Iter 0100 | Total Loss 0.368153\n",
      "Iter 0150 | Total Loss 0.367094\n",
      "Iter 0200 | Total Loss 0.367331\n",
      "Iter 0250 | Total Loss 0.366786\n",
      "Iter 0300 | Total Loss 0.367265\n",
      "Iter 0350 | Total Loss 0.366209\n",
      "Iter 0400 | Total Loss 0.366605\n",
      "Iter 0450 | Total Loss 0.366655\n",
      "Iter 0500 | Total Loss 0.365990\n",
      "Iter 0550 | Total Loss 0.365696\n",
      "Iter 0600 | Total Loss 0.366063\n",
      "Iter 0650 | Total Loss 0.365793\n",
      "Iter 0700 | Total Loss 0.365589\n",
      "Iter 0750 | Total Loss 0.365190\n",
      "Iter 0800 | Total Loss 0.365098\n",
      "Iter 0850 | Total Loss 0.365276\n",
      "Iter 0900 | Total Loss 0.365149\n",
      "Iter 0950 | Total Loss 0.365000\n",
      "Iter 1000 | Total Loss 0.364392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3644), 1356.3055098056793)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_utils.training_loop(niters, \n",
    "                    dat_folds[0], \n",
    "                    batch_size, \n",
    "                    optimizer, \n",
    "                    test_freq, \n",
    "                    enc_dec_ode_loss_func, \n",
    "                    enc_dec_ode_save_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (fc1): Linear(in_features=10, out_features=20, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_path = 'models/enc-dec-{}.pth'\n",
    "\n",
    "encoder = EncoderLSTM(4, 50, 10)\n",
    "encoder.load_state_dict(torch.load(model_path.format('encoder')))\n",
    "encoder.eval()\n",
    "\n",
    "latent_ode = ode_models.ODEFunc0(dim_y=10)\n",
    "latent_ode.load_state_dict(torch.load(model_path.format('ode')))\n",
    "latent_ode.eval()\n",
    "\n",
    "decoder = Decoder(10, 4, 20)\n",
    "decoder.load_state_dict(torch.load(model_path.format('decoder')))\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3643, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_ode_loss = enc_dec_ode_loss_func(*training_utils.get_all(dat_folds[0]['test']))\n",
    "latent_ode_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FS Latent ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 2000\n",
    "batch_size = 500\n",
    "step_size = 1./12\n",
    "test_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(4, 50, 10)\n",
    "fs_ode = ode_models.FSODE(input_dim=10, hidden_dim=50)\n",
    "decoder = Decoder(10, 4, 20)\n",
    "\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(fs_ode.parameters()) + list(decoder.parameters()), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_fs_ode_loss_func(t, y, y0, t_mask, y_mask, eids):\n",
    "    init_cond = encoder(y[:1, ...], t[:1])\n",
    "    # y_mask[:2, ...] = False\n",
    "    latent_y = dto(fs_ode, init_cond, t, method='euler_par', options={'step_size': step_size})\n",
    "    pred_y = decoder(latent_y)\n",
    "    loss = torch.mean(torch.abs(pred_y[y_mask] - y[y_mask]))\n",
    "    return loss\n",
    "\n",
    "def enc_dec_fs_ode_save_func():\n",
    "    model_path = 'models/enc-dec-fs-{}.pth'\n",
    "    torch.save(encoder.state_dict(), model_path.format('encoder'))\n",
    "    torch.save(decoder.state_dict(), model_path.format('decoder'))\n",
    "    torch.save(fs_ode.state_dict(), model_path.format('ode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0050 | Total Loss 0.368517\n",
      "Iter 0100 | Total Loss 0.368873\n",
      "Iter 0150 | Total Loss 0.368728\n",
      "Iter 0200 | Total Loss 0.368591\n",
      "Iter 0250 | Total Loss 0.367557\n",
      "Iter 0300 | Total Loss 0.369183\n",
      "Iter 0350 | Total Loss 0.368086\n",
      "Iter 0400 | Total Loss 0.367389\n",
      "Iter 0450 | Total Loss 0.367729\n",
      "Iter 0500 | Total Loss 0.366998\n",
      "Iter 0550 | Total Loss 0.367249\n",
      "Iter 0600 | Total Loss 0.366565\n",
      "Iter 0650 | Total Loss 0.366378\n",
      "Iter 0700 | Total Loss 0.366370\n",
      "Iter 0750 | Total Loss 0.365667\n",
      "Iter 0800 | Total Loss 0.365757\n",
      "Iter 0850 | Total Loss 0.365701\n",
      "Iter 0900 | Total Loss 0.365846\n",
      "Iter 0950 | Total Loss 0.365699\n",
      "Iter 1000 | Total Loss 0.365524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3655), 1887.9040415287018)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_utils.training_loop(niters, \n",
    "                    dat_folds[0], \n",
    "                    batch_size, \n",
    "                    optimizer, \n",
    "                    test_freq, \n",
    "                    enc_dec_fs_ode_loss_func, \n",
    "                    enc_dec_fs_ode_save_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3654, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs_ode_loss = enc_dec_fs_ode_loss_func(*training_utils.get_all(dat_folds[0]['test']))\n",
    "fs_ode_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HO Latent ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "niters = 2000\n",
    "batch_size = 500\n",
    "step_size = 1./12\n",
    "test_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(4, 50, 10)\n",
    "ho_ode = ode_models.HigherOrderOdeNoInit(dim=5, order=2, hidden_size=50)\n",
    "decoder = Decoder(10, 4, 20)\n",
    "\n",
    "optimizer = optim.Adam(list(encoder.parameters()) + list(ho_ode.parameters()) + list(decoder.parameters()), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enc_dec_ho_ode_loss_func(t, y, y0, t_mask, y_mask, eids):\n",
    "    init_cond = encoder(y[:1, ...], t[:1])\n",
    "    # y_mask[:2, ...] = False\n",
    "    latent_y = dto(ho_ode, init_cond, t, method='euler_par', options={'step_size': step_size})\n",
    "    pred_y = decoder(latent_y)\n",
    "    loss = torch.mean(torch.abs(pred_y[y_mask] - y[y_mask]))\n",
    "    return loss\n",
    "\n",
    "def enc_dec_ho_ode_save_func():\n",
    "    model_path = 'models/enc-dec-ho-{}.pth'\n",
    "    torch.save(encoder.state_dict(), model_path.format('encoder'))\n",
    "    torch.save(decoder.state_dict(), model_path.format('decoder'))\n",
    "    torch.save(ho_ode.state_dict(), model_path.format('ode'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0050 | Total Loss 0.600317\n",
      "Iter 0100 | Total Loss 0.549817\n",
      "Iter 0150 | Total Loss 0.518479\n",
      "Iter 0200 | Total Loss 0.498119\n",
      "Iter 0250 | Total Loss 0.479272\n",
      "Iter 0300 | Total Loss 0.458242\n",
      "Iter 0350 | Total Loss 0.435501\n",
      "Iter 0400 | Total Loss 0.420988\n",
      "Iter 0450 | Total Loss 0.410365\n",
      "Iter 0500 | Total Loss 0.398915\n",
      "Iter 0550 | Total Loss 0.386060\n",
      "Iter 0600 | Total Loss 0.381689\n",
      "Iter 0650 | Total Loss 0.378942\n",
      "Iter 0700 | Total Loss 0.377669\n",
      "Iter 0750 | Total Loss 0.375717\n",
      "Iter 0800 | Total Loss 0.375116\n",
      "Iter 0850 | Total Loss 0.374617\n",
      "Iter 0900 | Total Loss 0.373847\n",
      "Iter 0950 | Total Loss 0.372793\n",
      "Iter 1000 | Total Loss 0.373130\n",
      "Iter 1050 | Total Loss 0.371686\n",
      "Iter 1100 | Total Loss 0.372228\n",
      "Iter 1150 | Total Loss 0.371427\n",
      "Iter 1200 | Total Loss 0.371782\n",
      "Iter 1250 | Total Loss 0.370440\n",
      "Iter 1300 | Total Loss 0.370212\n",
      "Iter 1350 | Total Loss 0.370274\n",
      "Iter 1400 | Total Loss 0.369955\n",
      "Iter 1450 | Total Loss 0.369631\n",
      "Iter 1500 | Total Loss 0.369783\n",
      "Iter 1550 | Total Loss 0.368594\n",
      "Iter 1600 | Total Loss 0.368885\n",
      "Iter 1650 | Total Loss 0.369709\n",
      "Iter 1700 | Total Loss 0.368749\n",
      "Iter 1750 | Total Loss 0.369062\n",
      "Iter 1800 | Total Loss 0.368112\n",
      "Iter 1850 | Total Loss 0.368173\n",
      "Iter 1900 | Total Loss 0.367885\n",
      "Iter 1950 | Total Loss 0.367736\n",
      "Iter 2000 | Total Loss 0.367228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.3672), 2827.6357975006104)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_utils.training_loop(niters, \n",
    "                    dat_folds[0], \n",
    "                    batch_size, \n",
    "                    optimizer, \n",
    "                    test_freq, \n",
    "                    enc_dec_ho_ode_loss_func, \n",
    "                    enc_dec_ho_ode_save_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11, 1, 4])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Tensors must have same number of dimensions: got 2 and 3 at /opt/conda/conda-bld/pytorch_1570910687650/work/aten/src/TH/generic/THTensor.cpp:680",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-d6a915e50098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minit_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/alt/applic/user-maint/zq224/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-2f6a5d317604>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, y, t)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0my_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Tensors must have same number of dimensions: got 2 and 3 at /opt/conda/conda-bld/pytorch_1570910687650/work/aten/src/TH/generic/THTensor.cpp:680"
     ]
    }
   ],
   "source": [
    "init_cond = encoder(y[:1, ...], t[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t, y, y0, t_mask, y_mask, eids = training_utils.get_batch(dat_folds[0]['train'], batch_size=11)\n",
    "optimizer.zero_grad()\n",
    "init_cond = encoder(y[:2, ...], t[:2])\n",
    "# y_mask[:2, ...] = False\n",
    "latent_y = dto(latent_ode, init_cond, t, method='euler_par', options={'step_size': step_size})\n",
    "pred_y = decoder(latent_y)\n",
    "loss = torch.mean(torch.abs(pred_y[y_mask] - y[y_mask]))\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 11, 1, 4])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 11, 1, 10])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 11])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    t, y, y0, t_mask, y_mask, eids = training_utils.get_batch(dat_folds[0]['train'], batch_size=11)\n",
    "    opt.zero_grad()\n",
    "    init_cond = encoder(y[:2, ...], t[:2])\n",
    "    \n",
    "    pred_y = dto(latent_ode, init_cond, t, method='euler_par', options={'step_size': step_size})\n",
    "    \n",
    "    \n",
    "    \n",
    "    loss = torch.mean(torch.abs(init_cond.squeeze() - y0.squeeze()))\n",
    "    loss.backward()\n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006156797520816326"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000],\n",
       "        [1.8000, 1.2438, 2.1178, 0.8356, 3.2137, 0.6877, 0.9836, 3.9123, 1.0329,\n",
       "         2.1178, 1.0164],\n",
       "        [3.0247, 1.9425, 2.9260, 2.8493, 4.3616, 3.2630, 2.0904, 8.8082, 1.4384,\n",
       "         2.6301, 2.0795]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1, 4])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_cond.permute((1, 0, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 1, 4])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7702)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(func.init_cond_mat[:, :, 4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7975)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(func.init_cond_mat[:, :, :4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    t, y, y0, t_mask, y_mask, eids = get_all(dat_folds[0]['val'])\n",
    "    func.set_init_cond(eids)\n",
    "    init_zeros = torch.zeros_like(func.init_cond)\n",
    "\n",
    "    pred_y = dto(func, init_zeros, t, method='euler_par', options={'step_size': step_size})\n",
    "    pred_y_final = (pred_y  + func.init_cond)[..., :func.dim]\n",
    "\n",
    "    loss = torch.mean(torch.abs(pred_y_final[y_mask] - y[y_mask]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0748,  0.1314,  0.0741, -0.2251,  0.2084,  0.0000,  0.5503,  0.0850],\n",
       "        [-1.0692,  1.0429,  0.0741, -1.0025,  0.4438, -0.1469,  0.4507, -0.0696],\n",
       "        [-0.3912, -0.1421,  0.0182, -0.6570,  0.1905,  0.0000,  0.2935, -0.1618],\n",
       "        [-0.1370,  0.1314,  0.3538,  1.4160,  0.0709, -0.2287,  0.3508, -0.5056],\n",
       "        [ 0.6680, -1.3271, -2.1075,  0.8114, -0.8754, -0.4185,  2.8251, -0.9914]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.init_cond[:5,0, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0748,  0.1314,  0.0741, -0.2251],\n",
       "        [-1.0692,  1.0429,  0.0741, -1.0025],\n",
       "        [-0.3912, -0.1421,  0.0182, -0.6570],\n",
       "        [-0.1370,  0.1314,  0.3538,  1.4160],\n",
       "        [ 0.6680, -1.3271, -2.1075,  0.8114]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0[:5,0, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0748,  0.1314,  0.0741, -0.2251],\n",
       "        [ 0.2966,  0.3205,  0.4992,  0.1014],\n",
       "        [ 0.3616,  0.3970,  0.5153,  0.2271],\n",
       "        [ 0.3939,  0.4275,  0.5174,  0.2844],\n",
       "        [ 0.4475,  0.4661,  0.5184,  0.3721]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y_final[:5, 0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0748,  0.1314,  0.0741, -0.2251],\n",
       "        [ 0.2867,  0.1314,  0.6335, -0.1387],\n",
       "        [ 0.3291,  0.1314, -0.1496,  0.0340],\n",
       "        [ 0.4985, -0.2332, -0.4853, -1.0889],\n",
       "        [-0.0523,  0.1314,  0.0741, -0.3115]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5, 0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0050 | Total Loss 0.426231\n",
      "Iter 0100 | Total Loss 0.597094\n",
      "Iter 0150 | Total Loss 0.583271\n",
      "Iter 0200 | Total Loss 1.400030\n",
      "Iter 0250 | Total Loss 0.738257\n",
      "Iter 0300 | Total Loss 0.420469\n",
      "Iter 0350 | Total Loss 0.611337\n",
      "Iter 0400 | Total Loss 0.443519\n",
      "Iter 0450 | Total Loss 0.491005\n",
      "Iter 0500 | Total Loss 0.892621\n",
      "Iter 0550 | Total Loss 0.768308\n",
      "Iter 0600 | Total Loss 0.525871\n",
      "Iter 0650 | Total Loss 0.453953\n",
      "Iter 0700 | Total Loss 1.970526\n",
      "Iter 0750 | Total Loss 0.988136\n",
      "Iter 0800 | Total Loss 2.543571\n",
      "Iter 0850 | Total Loss 1.738551\n",
      "Iter 0900 | Total Loss 0.450546\n",
      "Iter 0950 | Total Loss 0.405119\n",
      "Iter 1000 | Total Loss 0.566877\n",
      "Iter 1050 | Total Loss 0.449112\n",
      "Iter 1100 | Total Loss 0.478447\n",
      "Iter 1150 | Total Loss 0.420794\n",
      "Iter 1200 | Total Loss 0.375969\n",
      "Iter 1250 | Total Loss 0.641680\n",
      "Iter 1300 | Total Loss 0.411319\n",
      "Iter 1350 | Total Loss 0.385653\n",
      "Iter 1400 | Total Loss 0.409560\n",
      "Iter 1450 | Total Loss 0.422992\n",
      "Iter 1500 | Total Loss 0.390408\n"
     ]
    }
   ],
   "source": [
    "ii = 0\n",
    "\n",
    "start = time.time()\n",
    "for itr in range(1, niters + 1):\n",
    "    \n",
    "    t, y, y0, t_mask, y_mask, eids = get_batch(dat_dict, batch_size, itr+500)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    func.set_init_cond(eids)\n",
    "    init_zeros = torch.zeros_like(func.init_cond)\n",
    "    \n",
    "    pred_y = dto(func, init_zeros, t, method='euler_par', options={'step_size': step_size})\n",
    "    pred_y_final = (pred_y  + func.init_cond)[..., :func.dim]\n",
    "    \n",
    "    loss = torch.mean(torch.abs(pred_y_final[y_mask] - y[y_mask]))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if itr % test_freq == 0:\n",
    "        with torch.no_grad():\n",
    "            print('Iter {:04d} | Total Loss {:.6f}'.format(itr, loss.item()))\n",
    "            ii += 1\n",
    "\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/cprd_ho2.pth'\n",
    "torch.save(func.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func1 = ode_models.HigherOrderOde(dat_dict,batch_size=batch_size, dim=4, order=2, hidden_size=50)\n",
    "# func1.load_state_dict(torch.load(model_path))\n",
    "# func1.eval()\n",
    "# func1.init_cond_mat.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.17963133653005"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(end-start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr=498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, y, y0, t_mask, y_mask, eids = get_batch(dat_dict, batch_size, itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8423206'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': tensor([0.0000, 3.1205, 4.7178]),\n",
       " 'x': tensor([[[[0.4524, 2.7864, 2.9481, 1.3142]]],\n",
       " \n",
       " \n",
       "         [[[0.1569, 1.9626, 1.4286, 0.2870]]],\n",
       " \n",
       " \n",
       "         [[[0.7479, 1.6880, 1.6537, 0.8006]]]])}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_dict['14260073']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4524, 2.7864, 2.9481, 1.3142]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y0[0, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5238)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.array([func.eid_to_id[x] for x in eids])\n",
    "id_torch = torch.from_numpy(idx)\n",
    "#  self.init_cond = self.init_cond_mat[id_torch, ...]\n",
    "id_torch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4512,  2.7882,  2.9492,  1.3124, -0.0830, -0.2435, -0.4933, -0.3075]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.init_cond_mat[5238]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1238, -0.6072, -1.0318,  0.8855, -0.1671,  0.3526,  0.2405, -0.1994]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.init_cond[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7010, 1, 8])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.init_cond_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_untrain = ode_models.HigherOrderOde(dat_dict,batch_size=batch_size, dim=4, order=2, hidden_size=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.abs(func.init_cond_mat[..., :4] - func_untrain.init_cond_mat[..., :4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1154,  0.6854,  1.9798,  2.4262],\n",
       "        [-1.1180,  0.0436, -0.2041,  0.8874],\n",
       "        [ 0.4524, -0.5087, -0.8788, -0.2265],\n",
       "        [ 2.0107, -0.1345,  0.5280,  0.5414],\n",
       "        [ 0.1557, -0.3421,  0.3031, -0.1577]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.init_cond_mat[:5, 0, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0964,  0.6812,  1.9914,  2.4268],\n",
       "        [-1.1096,  0.0405, -0.2035,  0.8862],\n",
       "        [ 0.4524, -0.5087, -0.8788, -0.2265],\n",
       "        [ 2.0144, -0.1426,  0.5282,  0.5438],\n",
       "        [ 0.1569, -0.3256,  0.3031, -0.1409]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_untrain.init_cond_mat[:5, 0, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6105,  0.4214,  2.3506, -0.3816],\n",
       "        [-0.0939,  0.3392,  0.6464, -0.7820],\n",
       "        [-1.3570,  0.3796, -0.4085,  0.0888],\n",
       "        [-0.0635, -0.4829, -0.3766, -0.1169],\n",
       "        [-0.1704,  0.3768, -0.1794,  0.4243]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.init_cond_mat[:5, 0, -4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5927,  0.4283,  2.3702, -0.4005],\n",
       "        [-0.0888,  0.3464,  0.6390, -0.7918],\n",
       "        [-1.3570,  0.3796, -0.4085,  0.0888],\n",
       "        [-0.0617, -0.4903, -0.3837, -0.1250],\n",
       "        [-0.1813,  0.3930, -0.1933,  0.4411]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_untrain.init_cond_mat[:5, 0, -4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 500\n",
      "Processed: 1000\n",
      "Processed: 1500\n",
      "Processed: 2000\n",
      "Processed: 2500\n",
      "Processed: 3000\n",
      "Processed: 3500\n",
      "Processed: 4000\n",
      "Processed: 4500\n",
      "Processed: 5000\n",
      "Processed: 5500\n",
      "Processed: 6000\n",
      "Processed: 6500\n",
      "Processed: 7000\n"
     ]
    }
   ],
   "source": [
    "n_processed = 0\n",
    "step_size = 1./12\n",
    "\n",
    "s = time.time()\n",
    "for eid, v in dat_dict.items():\n",
    "    t = v['t']\n",
    "    x = v['x']\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    func.set_init_cond([eid])\n",
    "    init_zeros = torch.zeros_like(func.init_cond)\n",
    "    \n",
    "#     pred_y = odeint(func, init_zeros, t)\n",
    "    pred_y = dto(func, init_zeros, t, method='euler', options={'step_size': step_size})\n",
    "    \n",
    "    pred_y_final = pred_y  + func.init_cond\n",
    "    loss = torch.mean(torch.abs(pred_y_final[..., :func.dim] - x))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    n_processed += 1\n",
    "    \n",
    "    if n_processed % 500 == 0:\n",
    "        print('Processed:', n_processed)\n",
    "#     if n_processed > 100:\n",
    "#         break\n",
    "e = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175.36765384674072"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e - s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_zeros.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[100.0000,  91.0000, 128.0000,   6.4000, -11.1027,  -1.3878,   5.5513,\n",
       "           -2.6369]]], requires_grad=True)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.init_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 8])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func.init_cond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 % 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = odeint(func, init_zeros, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HigherOrderOde' object has no attribute 'set_init_cond'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-66047e822195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_init_cond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0minit_zeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/alt/applic/user-maint/zq224/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 585\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HigherOrderOde' object has no attribute 'set_init_cond'"
     ]
    }
   ],
   "source": [
    "v = dat_dict['1092']\n",
    "\n",
    "t = v['t']\n",
    "x = v['x']\n",
    "\n",
    "optimizer.zero_grad()\n",
    "func.set_init_cond(eid, t, x)\n",
    "init_zeros = torch.zeros_like(func.init_cond)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = odeint(func, init_zeros, t)\n",
    "pred_y_final = pred_y + func.init_cond\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.mean(torch.abs(pred_y_final[..., :func.dim] - x))\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, ...].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "eid_list = list(dat_dict.keys())\n",
    "eid_to_id = dict(zip(eid_list, range(len(eid_list))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': tensor([0.0000, 0.8548, 4.8959]),\n",
       " 'x': tensor([[[[ 86.0000,  86.0000, 174.0000,   7.6000]]],\n",
       " \n",
       " \n",
       "         [[[ 74.0000,  90.0000, 210.0000,   7.2000]]],\n",
       " \n",
       " \n",
       "         [[[ 88.0000,  82.0000, 140.0000,   5.6000]]]])}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_dict['1092']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': tensor([0.0000, 2.3781, 5.7644]),\n",
       " 'x': tensor([[[[ 62.0000,  79.0000, 135.0000,   5.8000]]],\n",
       " \n",
       " \n",
       "         [[[ 57.0000,  88.0000, 162.0000,   3.6000]]],\n",
       " \n",
       " \n",
       "         [[[ 74.0000,  78.0000, 146.0000,   4.8000]]]])}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_dict['3259']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 1, 4])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
